{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_PwBVB9C-cql","vg0qcAZf-irZ","DOhJ54PchV77","pGC83sGW9vzH","J2tL14BI-ulS"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Data Analysis"],"metadata":{"id":"JBhiid2t14BN"}},{"cell_type":"markdown","source":["### Train Data Analysis"],"metadata":{"id":"_PwBVB9C-cql"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read the train and validation data\n","\n","train_df = pd.read_csv('/content/sample_data/trainData.csv')\n","val_df = pd.read_csv('/content/sample_data/validationData.csv')\n","\n","\n","\n","# Display the first few rows of each dataset\n","print(\"Train Data:\")\n","print(train_df.head())\n","\n","print(\"\\nValidation Data:\")\n","print(val_df.head())\n"],"metadata":{"id":"GN7yCyrWzmDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","\n","# # Load the uploaded file\n","# file_path = '/mnt/data/trainData.csv'\n","# train_df = pd.read_csv(file_path)\n","\n","# Display basic information about the dataset\n","train_info = {\n","    \"head\": train_df.head(),\n","    \"info\": train_df.info(),\n","    \"describe\": train_df.describe(include=\"all\"),\n","    \"null_values\": train_df.isnull().sum(),\n","    \"columns\": train_df.columns.tolist()\n","}\n","\n","train_info\n"],"metadata":{"id":"1CJlwHfwz8-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert 'data' column to datetime\n","train_df['data'] = pd.to_datetime(train_df['data'])\n","\n","# Handle missing values: Fill missing NO2 with the median for each station\n","train_df['NO2'] = train_df.groupby('nom_estacio')['NO2'].transform(lambda x: x.fillna(x.median()))\n","\n","# Investigate and handle negative NO2 values (e.g., set them to NaN or replace them with a minimum threshold)\n","train_df.loc[train_df['NO2'] < 0, 'NO2'] = None\n","train_df['NO2'] = train_df['NO2'].fillna(train_df['NO2'].median())\n","\n","# Summary statistics after preprocessing\n","summary_stats = train_df['NO2'].describe()\n","\n","# Analyze NO2 levels by station and hour\n","station_hourly_mean = train_df.groupby(['nom_estacio', 'hour'])['NO2'].mean().reset_index()\n","\n","# Plot distribution of NO2 levels\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 6))\n","train_df['NO2'].hist(bins=50, edgecolor='black')\n","plt.title(\"Distribution of NO2 Levels\")\n","plt.xlabel(\"NO2 Concentration\")\n","plt.ylabel(\"Frequency\")\n","plt.show()\n","\n","# Analyze NO2 trends per station\n","station_trends = train_df.groupby(['nom_estacio', train_df['data'].dt.month])['NO2'].mean().reset_index()\n","\n","print(summary_stats)\n","print(station_hourly_mean.head())\n"],"metadata":{"id":"GNQVu2Mw0KHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","\n","\n","# Ensure 'data' is in datetime format\n","train_df['data'] = pd.to_datetime(train_df['data'], errors='coerce')\n","\n","# Combine 'data' and 'hour' into a single datetime column\n","train_df['datetime'] = train_df['data'] + pd.to_timedelta(train_df['hour'] - 1, unit='h')\n","\n","# Set up the plot\n","plt.figure(figsize=(14, 8))\n","\n","# Plot NO2 levels for each station\n","stations = train_df['nom_estacio'].unique()\n","for station in stations:\n","    station_data = train_df[train_df['nom_estacio'] == station]\n","    plt.plot(station_data['datetime'], station_data['NO2'], label=station, alpha=0.7)\n","\n","# Format the x-axis to show both day and hour\n","plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n","plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=30))\n","plt.xticks(rotation=45)\n","\n","# Labels and legend\n","plt.title(\"NO2 Levels Over Time for Each Station\")\n","plt.xlabel(\"Date and Hour\")\n","plt.ylabel(\"NO2 Concentration\")\n","plt.legend(title=\"Station\")\n","plt.grid(True)\n","\n","# Display the plot\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"fkh5-yYz1Ics"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up subplots for each station\n","stations = train_df['nom_estacio'].unique()\n","fig, axes = plt.subplots(len(stations), 1, figsize=(14, 20), sharex=True)\n","\n","for i, station in enumerate(stations):\n","    station_data = train_df[train_df['nom_estacio'] == station]\n","    axes[i].plot(station_data['datetime'], station_data['NO2'], label=f\"{station} NO2 Levels\", alpha=0.7)\n","    avg_NO2 = station_data['NO2'].mean()\n","    axes[i].axhline(avg_NO2, color='red', linestyle='--', label=f\"Average NO2 ({avg_NO2:.2f})\")\n","    axes[i].set_title(f\"NO2 Levels Over Time for {station}\")\n","    axes[i].set_ylabel(\"NO2 Concentration\")\n","    axes[i].legend()\n","    axes[i].grid(True)\n","\n","# Common x-axis formatting\n","plt.xlabel(\"Date and Hour\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"LvWiVGs51cpl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure 'data' is in datetime format\n","train_df['data'] = pd.to_datetime(train_df['data'], errors='coerce')\n","\n","# Combine 'data' and 'hour' into a single datetime column\n","train_df['datetime'] = train_df['data'] + pd.to_timedelta(train_df['hour'] - 1, unit='h')\n","\n","# Set up subplots for each station to visualize missing values\n","stations = train_df['nom_estacio'].unique()\n","fig, axes = plt.subplots(len(stations), 1, figsize=(14, 20), sharex=True)\n","\n","for i, station in enumerate(stations):\n","    station_data = train_df[train_df['nom_estacio'] == station]\n","    missing_data = station_data[station_data['NO2'].isnull()]\n","\n","    axes[i].scatter(\n","        missing_data['datetime'],\n","        [0] * len(missing_data),  # Dummy y-values for visualization\n","        color='red',\n","        label='Missing Values',\n","        alpha=0.6\n","    )\n","\n","    axes[i].set_title(f\"Missing NO2 Values Over Time for {station}\")\n","    axes[i].set_ylabel(\"Missing Values (Indicator)\")\n","    axes[i].legend()\n","    axes[i].grid(True)\n","\n","# Common x-axis formatting\n","plt.xlabel(\"Date and Hour\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"ADCiIiQu123f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check if there are NaN values in the 'NO2' column\n","print(\"Missing values per station:\")\n","print(train_df[train_df['NO2'].isnull()]['nom_estacio'].value_counts())\n","\n","# Ensure 'datetime' column is correctly created\n","train_df['data'] = pd.to_datetime(train_df['data'], errors='coerce')\n","train_df['datetime'] = train_df['data'] + pd.to_timedelta(train_df['hour'] - 1, unit='h')\n","\n","# Filter data for missing NO2 values\n","missing_data = train_df[train_df['NO2'].isnull()]\n","if missing_data.empty:\n","    print(\"No missing values found for NO2 in the dataset.\")\n","else:\n","    # Plot missing values per station\n","    fig, axes = plt.subplots(len(stations), 1, figsize=(14, 20), sharex=True)\n","\n","    for i, station in enumerate(stations):\n","        station_data = missing_data[missing_data['nom_estacio'] == station]\n","\n","        if not station_data.empty:\n","            axes[i].scatter(\n","                station_data['datetime'],\n","                [0] * len(station_data),  # Dummy y-values for visualization\n","                color='red',\n","                label='Missing Values',\n","                alpha=0.6\n","            )\n","        else:\n","            print(f\"No missing NO2 values for station: {station}\")\n","\n","        axes[i].set_title(f\"Missing NO2 Values Over Time for {station}\")\n","        axes[i].set_ylabel(\"Missing Values (Indicator)\")\n","        axes[i].legend()\n","        axes[i].grid(True)\n","\n","    # Common x-axis formatting\n","    plt.xlabel(\"Date and Hour\")\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","    plt.show()\n"],"metadata":{"id":"kE1mOHlN2lsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter missing values and check the stations\n","missing_data = train_df[train_df['NO2'].isnull()]\n","stations = train_df['nom_estacio'].unique()\n","\n","fig, axes = plt.subplots(len(stations), 1, figsize=(14, 20), sharex=True)\n","\n","for i, station in enumerate(stations):\n","    station_data = missing_data[missing_data['nom_estacio'] == station]\n","\n","    if not station_data.empty:\n","        axes[i].scatter(\n","            station_data['datetime'],\n","            [0] * len(station_data),  # Dummy y-values for visualization\n","            color='red',\n","            label='Missing Values',\n","            alpha=0.6\n","        )\n","    else:\n","        axes[i].text(0.5, 0.5, \"No Missing Values\", transform=axes[i].transAxes, ha=\"center\")\n","\n","    axes[i].set_title(f\"Missing NO2 Values Over Time for {station}\")\n","    axes[i].set_ylabel(\"Missing Values (Indicator)\")\n","    axes[i].legend()\n","    axes[i].grid(True)\n","\n","# Common x-axis formatting\n","plt.xlabel(\"Date and Hour\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"Kqc3cdTA28KZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reload the original dataset\n","train_df = pd.read_csv('/content/sample_data/trainData.csv')\n","\n","# Ensure 'data' is in datetime format\n","train_df['data'] = pd.to_datetime(train_df['data'], errors='coerce')\n","\n","# Combine 'data' and 'hour' into a single datetime column\n","train_df['datetime'] = train_df['data'] + pd.to_timedelta(train_df['hour'] - 1, unit='h')\n","\n","# Filter missing values from the original dataset\n","missing_data = train_df[train_df['NO2'].isnull()]\n","stations = train_df['nom_estacio'].unique()\n","\n","# Plot missing values per station\n","fig, axes = plt.subplots(len(stations), 1, figsize=(14, 20), sharex=True)\n","\n","for i, station in enumerate(stations):\n","    station_data = missing_data[missing_data['nom_estacio'] == station]\n","\n","    if not station_data.empty:\n","        axes[i].scatter(\n","            station_data['datetime'],\n","            [0] * len(station_data),  # Dummy y-values for visualization\n","            color='red',\n","            label='Missing Values',\n","            alpha=0.6\n","        )\n","    else:\n","        axes[i].text(0.5, 0.5, \"No Missing Values\", transform=axes[i].transAxes, ha=\"center\")\n","\n","    axes[i].set_title(f\"Missing NO2 Values Over Time for {station}\")\n","    axes[i].set_ylabel(\"Missing Values (Indicator)\")\n","    axes[i].legend()\n","    axes[i].grid(True)\n","\n","# Common x-axis formatting\n","plt.xlabel(\"Date and Hour\")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"Bu4CcZVj8ttG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count the number of missing NO2 values for each station\n","missing_per_station = train_df[train_df['NO2'].isnull()]['nom_estacio'].value_counts()\n","\n","# Display the counts\n","print(\"Missing NO2 values per station:\")\n","print(missing_per_station)\n","\n"],"metadata":{"id":"CWR8e91A9Yje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","train_df = pd.read_csv('/content/sample_data/trainData.csv')\n","\n","# Count missing and non-missing NO2 values per station\n","missing_counts = train_df.groupby('nom_estacio')['NO2'].apply(lambda x: x.isnull().sum())\n","existing_counts = train_df.groupby('nom_estacio')['NO2'].apply(lambda x: x.notnull().sum())\n","\n","# Create a DataFrame for fractions\n","fraction_df = pd.DataFrame({\n","    'Missing': missing_counts,\n","    'Existing': existing_counts\n","})\n","fraction_df['Total'] = fraction_df['Missing'] + fraction_df['Existing']\n","fraction_df['Missing Fraction'] = fraction_df['Missing'] / fraction_df['Total']\n","fraction_df['Existing Fraction'] = fraction_df['Existing'] / fraction_df['Total']\n","\n","# Plot the fractions as a bar chart\n","fraction_df[['Missing Fraction', 'Existing Fraction']].plot(\n","    kind='bar',\n","    stacked=True,\n","    figsize=(10, 6),\n","    color=['red', 'green'],\n","    alpha=0.7\n",")\n","\n","plt.title('Fraction of Missing and Existing NO2 Values per Station')\n","plt.ylabel('Fraction')\n","plt.xlabel('Station')\n","plt.xticks(rotation=45)\n","plt.legend(title='Value Type')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"K0kWaOo79qsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a bar plot for real numbers of missing and existing values\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Bar width for side-by-side bars\n","bar_width = 0.4\n","stations = fraction_df.index\n","x = range(len(stations))\n","\n","# Plot the data\n","ax.bar(x, fraction_df['Existing'], width=bar_width, label='Existing', color='green', alpha=0.7)\n","ax.bar([pos + bar_width for pos in x], fraction_df['Missing'], width=bar_width, label='Missing', color='red', alpha=0.7)\n","\n","# Add labels and legend\n","ax.set_xticks([pos + bar_width / 2 for pos in x])\n","ax.set_xticklabels(stations, rotation=45)\n","ax.set_ylabel('Count')\n","ax.set_title('Count of Missing and Existing NO2 Values per Station')\n","ax.legend(title='Value Type')\n","ax.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"uU8XxGDv9-eZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Validation Data Analysis"],"metadata":{"id":"vg0qcAZf-irZ"}},{"cell_type":"code","source":["# Display basic information about the dataset\n","val_info = {\n","    \"head\": val_df.head(),\n","    \"info\": val_df.info(),\n","    \"describe\": val_df.describe(include=\"all\"),\n","    \"null_values\": val_df.isnull().sum(),\n","    \"columns\": val_df.columns.tolist()\n","}\n","\n","val_info"],"metadata":{"id":"FP5A0dCw-pm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Load the validation dataset\n","# val_file_path = '/mnt/data/validationData.csv'\n","# val_df = pd.read_csv(val_file_path)\n","\n","# Count missing and existing NO2 values\n","val_missing_count = val_df['NO2'].isnull().sum()\n","val_existing_count = val_df['NO2'].notnull().sum()\n","\n","# Create a bar plot for missing and existing values\n","fig, ax = plt.subplots(figsize=(6, 6))\n","\n","# Bar labels\n","labels = ['Existing', 'Missing']\n","values = [val_existing_count, val_missing_count]\n","colors = ['green', 'red']\n","\n","# Plot the data\n","ax.bar(labels, values, color=colors, alpha=0.7)\n","\n","# Add labels and title\n","ax.set_ylabel('Count')\n","ax.set_title('Count of Missing and Existing NO2 Values (Validation)')\n","ax.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"wFLJ3LOd_zqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Ensure 'data' is in datetime format\n","val_df['data'] = pd.to_datetime(val_df['data'], errors='coerce')\n","\n","# Combine 'data' and 'hour' into a single datetime column\n","val_df['datetime'] = val_df['data'] + pd.to_timedelta(val_df['hour'] - 1, unit='h')\n","\n","# Set up a single plot\n","fig, ax = plt.subplots(figsize=(14, 8))\n","\n","# Plot NO2 levels over time\n","ax.plot(val_df['datetime'], val_df['NO2'], label='NO2 Levels', alpha=0.7)\n","\n","# Calculate and plot the average NO2 as a horizontal line\n","avg_NO2 = val_df['NO2'].mean()\n","ax.axhline(avg_NO2, color='red', linestyle='--', label=f\"Average NO2 ({avg_NO2:.2f})\")\n","\n","# Add title, labels, and legend\n","ax.set_title(\"NO2 Levels Over Time (Validation Dataset)\")\n","ax.set_xlabel(\"Date and Hour\")\n","ax.set_ylabel(\"NO2 Concentration\")\n","ax.legend()\n","ax.grid(True)\n","\n","# Format x-axis for better readability\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"7vWcH5zYA1gn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TrainData : dealing with `NaN`"],"metadata":{"id":"DOhJ54PchV77"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the dataset to examine it\n","file_path = '/content/sample_data/trainData.csv'\n","data = pd.read_csv(file_path)\n","\n","# Display the first few rows of the dataset to understand its structure\n","data.head()\n"],"metadata":{"id":"_wTwJALDGcae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select only numeric columns for negative value counting\n","numeric_data = data.select_dtypes(include=['number'])\n","\n","# Count negative values for each station (column)\n","negative_count = (numeric_data < 0).sum()\n","\n","# Combine the results in a dataframe for display\n","summary = pd.DataFrame({\n","    'NaN Count': nan_count,\n","    'Negative Count': negative_count\n","})\n","\n","#tools.display_dataframe_to_user(name=\"NaN and Negative Values Summary\", dataframe=summary)"],"metadata":{"id":"6RhkDAO935FC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace negative values with NaN in the numeric columns\n","numeric_data[numeric_data < 0] = float('nan')\n","\n","# Count NaN values again after replacement\n","nan_count_updated = numeric_data.isna().sum()\n","\n","# Combine the updated results in a dataframe for display\n","summary_updated = pd.DataFrame({\n","    'NaN Count': nan_count_updated,\n","    'Negative Count': negative_count  # Using the previous negative count\n","})"],"metadata":{"id":"r3IlZmtx4X54"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot the processed dataset (replace negative values with NaN) as a temporary series for each station\n","# We will use the numeric data after replacing negative values\n","processed_data_series = numeric_data.mean()  # Plotting the mean of each column as a simple series\n","\n","# Plotting the series\n","plt.figure(figsize=(10, 6))\n","processed_data_series.plot(kind='bar')\n","plt.title('Processed Data - Mean Values After Replacing Negatives with NaN')\n","plt.xlabel('Station')\n","plt.ylabel('Mean Value')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"PxThei884q_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Convert the 'data' column to datetime and create a 'datetime' column combining 'data' and 'hour'\n","data['datetime'] = pd.to_datetime(data['data']) + pd.to_timedelta(data['hour'] - 1, unit='h')\n","\n","# Plotting NO2 concentration as a time series for each station\n","stations = data['nom_estacio'].unique()\n","\n","plt.figure(figsize=(12, 6))\n","\n","for station in stations:\n","    station_data = data[data['nom_estacio'] == station]\n","    plt.plot(station_data['datetime'], station_data['NO2'], label=station)\n","\n","plt.title('NO2 Concentration Time Series for Different Stations')\n","plt.xlabel('Time')\n","plt.ylabel('NO2 Concentration')\n","plt.legend(title='Station')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"ORgb9rg-5NlV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Re-load the dataset and preprocess the NO2 column by replacing negative values with NaN\n","data = pd.read_csv(file_path)\n","\n","# Create a new processed version of the data where negative NO2 values are replaced with NaN\n","data_processed = data.copy()\n","data_processed['NO2'] = data_processed['NO2'].apply(lambda x: x if x >= 0 else float('nan'))\n","\n","# Create datetime column by combining 'data' (date) and 'hour'\n","data_processed['datetime'] = pd.to_datetime(data_processed['data']) + pd.to_timedelta(data_processed['hour'] - 1, unit='h')\n","\n","# Now plot the processed data (1 plot for each of the 5 stations)\n","stations = data_processed['nom_estacio'].unique()\n","\n","plt.figure(figsize=(12, 8))\n","\n","# Generate a plot for each station\n","for i, station in enumerate(stations, start=1):\n","    plt.subplot(3, 2, i)  # Create a subplot for each station\n","    station_data = data_processed[data_processed['nom_estacio'] == station]\n","    plt.plot(station_data['datetime'], station_data['NO2'], label=station)\n","    plt.title(f'NO2 Concentration - {station}')\n","    plt.xlabel('Time')\n","    plt.ylabel('NO2 Concentration')\n","    plt.xticks(rotation=45)\n","    plt.tight_layout()\n","\n","plt.show()\n"],"metadata":{"id":"sEnup96C6WTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Round the datetime to the nearest hour for consistency (if necessary)\n","data_processed['datetime_hour'] = data_processed['datetime'].dt.floor('H')\n","\n","# Step 2: Compute the mean NO2 concentration for each unique hour across all stations\n","hourly_mean = (\n","    data_processed.groupby('datetime_hour')['NO2']\n","    .mean()\n","    .reset_index()\n","    .rename(columns={'NO2': 'NO2_mean_hourly'})\n",")\n","\n","# Step 3: Extract the date from the datetime_hour column\n","hourly_mean['date'] = hourly_mean['datetime_hour'].dt.date\n","\n","# Step 4: Optionally reduce to one row per day (daily mean of hourly means)\n","daily_mean = (\n","    hourly_mean.groupby('date')['NO2_mean_hourly']\n","    .mean()\n","    .reset_index()\n","    .rename(columns={'NO2_mean_hourly': 'NO2_daily_mean'})\n",")\n","\n","# Step 5: Plot the daily mean NO2 concentration time series\n","plt.figure(figsize=(12, 6))\n","plt.plot(daily_mean['date'], daily_mean['NO2_daily_mean'], label='Daily Mean NO2 Across All Stations', color='b')\n","plt.title('Daily Mean NO2 Concentration Time Series Across All Stations')\n","plt.xlabel('Date')\n","plt.ylabel('Daily Mean NO2 Concentration')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"wVkInOtC65ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_processed"],"metadata":{"id":"vgH_ae1ORhvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Round the datetime to the nearest hour for consistency\n","data_processed['datetime_hour'] = data_processed['datetime'].dt.floor('H')\n","\n","# Step 2: Compute the mean NO2 concentration for each unique hour across all stations\n","hourly_mean = (\n","    data_processed.groupby('datetime_hour')['NO2']\n","    .mean()\n","    .reset_index()\n","    .rename(columns={'NO2': 'NO2_mean_hourly'})\n",")\n","\n","# Step 3: Replace the original dataset with the reduced dataset\n","data_processed = hourly_mean.copy()\n","\n","# Step 4: Verify the reduced dataset\n","print(data_processed.head())\n","\n","# Step 5: Plot the hourly mean NO2 concentration time series (optional)\n","plt.figure(figsize=(12, 6))\n","plt.plot(data_processed['datetime_hour'], data_processed['NO2_mean_hourly'], label='Hourly Mean NO2 Across All Stations', color='b')\n","plt.title('Hourly Mean NO2 Concentration Time Series Across All Stations')\n","plt.xlabel('Datetime Hour')\n","plt.ylabel('Hourly Mean NO2 Concentration')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"CyEgn5fITDqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_processed"],"metadata":{"id":"oHoPsfmzTLur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count NaN values in the NO2 column of the processed data\n","nan_count = data_processed['NO2_mean_hourly'].isna().sum()\n","\n","print(f\"Number of NaN values in NO2 column: {nan_count}\")"],"metadata":{"id":"YKdF3lR58F5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count valid (non-NaN) and NaN values for the NO2 column\n","valid_count = data_processed['NO2_mean_hourly'].notna().sum()\n","nan_count = data_processed['NO2_mean_hourly'].isna().sum()\n","\n","# Create a bar plot showing NaN vs valid counts\n","plt.figure(figsize=(8, 6))\n","plt.bar(['Valid', 'NaN'], [valid_count, nan_count], color=['green', 'red'])\n","plt.title('Count of Valid vs NaN NO2 Values')\n","plt.ylabel('Count')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QUpMIxf18RO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform linear interpolation to fill NaN values in the NO2 column\n","data_processed['NO2_interpolated'] = data_processed['NO2_mean_hourly'].interpolate(method='linear')\n","\n","# Plot the data with interpolated NO2 values\n","plt.figure(figsize=(12, 6))\n","plt.plot(data_processed['datetime_hour'], data_processed['NO2_interpolated'], label='Interpolated NO2', color='b')\n","plt.title('Interpolated NO2 Concentration Time Series Across All Stations')\n","plt.xlabel('Time')\n","plt.ylabel('Interpolated NO2 Concentration')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"BAppwjka8dXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform linear interpolation to fill NaN values in the NO2 column\n","data_processed['NO2_interpolated'] = data_processed['NO2_mean_hourly'].interpolate(method='linear')\n","\n","# Count NaN values after interpolation (should be 0 if interpolation worked correctly)\n","nan_count_interpolated = data_processed['NO2_interpolated'].isna().sum()\n","\n","nan_count_interpolated"],"metadata":{"id":"AloRgiui80Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count valid (non-NaN) and NaN values for the interpolated NO2 column\n","valid_count_interpolated = data_processed['NO2_interpolated'].notna().sum()\n","nan_count_interpolated = data_processed['NO2_interpolated'].isna().sum()\n","\n","# Create a bar plot showing NaN vs valid counts for the interpolated data\n","plt.figure(figsize=(8, 6))\n","plt.bar(['Valid', 'NaN'], [valid_count_interpolated, nan_count_interpolated], color=['green', 'red'])\n","plt.title('Count of Valid vs NaN NO2 Interpolated Values')\n","plt.ylabel('Count')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"w7enisws873H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the preprocessed dataset to a CSV file in the current directory\n","preprocessed_file_path = '/content/sample_data/preprocessed_data.csv'\n","data_processed.to_csv(preprocessed_file_path, index=False)\n","\n","preprocessed_file_path\n"],"metadata":{"id":"QmXA90pX89dR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ValData : dealing with `NaN`"],"metadata":{"id":"pGC83sGW9vzH"}},{"cell_type":"code","source":["# Load the validation dataset\n","validation_data_path = '/content/sample_data/validationData.csv'\n","validation_data = pd.read_csv(validation_data_path)\n","\n","# Perform the same preprocessing on the validation data (replace negative values with NaN, then interpolate)\n","validation_data['NO2'] = validation_data['NO2'].apply(lambda x: x if x >= 0 else float('nan'))\n","validation_data['NO2_interpolated'] = validation_data['NO2'].interpolate(method='linear')\n","\n","# Count NaN values after interpolation for validation data\n","nan_count_validation_interpolated = validation_data['NO2_interpolated'].isna().sum()\n","\n","nan_count_validation_interpolated\n"],"metadata":{"id":"VHAKZ6q292kZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the preprocessed validation dataset to a CSV file\n","preprocessed_validation_file_path = '/content/sample_data/preprocessed_validation_data.csv'\n","validation_data.to_csv(preprocessed_validation_file_path, index=False)\n","\n","preprocessed_validation_file_path\n"],"metadata":{"id":"uGDcOlT2-NxR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Deep Learning Model"],"metadata":{"id":"J2tL14BI-ulS"}},{"cell_type":"code","source":["# Read the train and validation data\n","train_data_processed = pd.read_csv('/content/sample_data/preprocessed_data.csv')\n","validation_data_processed = pd.read_csv('/content/sample_data/preprocessed_validation_data.csv')\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers as tfkl\n","import numpy as np\n","\n","# Display the first few rows of the train and validation datasets\n","train_head = train_data_processed.head()\n","validation_head = validation_data_processed.head()\n","\n","train_head, validation_head\n","# Remove the 'NO2' column and retain 'NO2_interpolated'\n","train_data_processed = train_data_processed.drop(columns=['NO2_mean_hourly'])\n","\n","# Display the updated validation dataset's head\n","print(\"Updated Train Dataset:\")\n","print(train_data_processed.head())\n","print(\"\\nShape of the Train Dataset:\", train_data_processed.shape)\n","\n","# Remove the 'NO2' column and retain 'NO2_interpolated'\n","validation_data_processed = validation_data_processed.drop(columns=['NO2'])\n","\n","# Display the updated validation dataset's head\n","print(\"Updated Validation Dataset:\")\n","print(validation_data_processed.head())\n","print(\"\\nShape of the Val Dataset:\", validation_data_processed.shape)"],"metadata":{"id":"hppAeoOu-xj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_sliding_windows(\n","    series: np.ndarray,\n","    input_length: int = 168,\n","    forecast_length: int = 24\n","):\n","    \"\"\"\n","    Given a 1D or 2D NumPy array `series` of shape (time, features),\n","    return X of shape (num_samples, input_length, features) and\n","    y of shape (num_samples, forecast_length, features).\n","\n","    If the series is 1D, it will be reshaped to (time, 1).\n","    \"\"\"\n","    if series.ndim == 1:\n","        # Reshape to (time, 1)\n","        series = series.reshape(-1, 1)\n","\n","    X_list, y_list = [], []\n","\n","    max_start = len(series) - input_length - forecast_length + 1\n","    for start_idx in range(max_start):\n","        end_idx = start_idx + input_length\n","        forecast_end_idx = end_idx + forecast_length\n","\n","        X_window = series[start_idx:end_idx]\n","        y_window = series[end_idx:forecast_end_idx]\n","\n","        X_list.append(X_window)\n","        y_list.append(y_window)\n","\n","    return np.array(X_list), np.array(y_list)\n"],"metadata":{"id":"7GY6xiIAa58f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_series = train_data_processed['NO2_interpolated'].values\n","\n","# Choose window sizes\n","input_length = 168\n","forecast_length = 24\n","\n","X_train, y_train = create_sliding_windows(\n","    series=train_series,\n","    input_length=input_length,\n","    forecast_length=forecast_length\n",")\n","\n","print(\"X_train shape:\", X_train.shape)  # (N, 168, 1) if only NO2\n","print(\"y_train shape:\", y_train.shape)  # (N, 24, 1)\n"],"metadata":{"id":"A6fcH-0McJ5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_series = validation_data_processed['NO2_interpolated'].values\n","\n","X_val, y_val = create_sliding_windows(\n","    series=val_series,\n","    input_length=input_length,\n","    forecast_length=forecast_length\n",")\n","\n","print(\"X_val shape:\", X_val.shape)\n","print(\"y_val shape:\", y_val.shape)\n"],"metadata":{"id":"GKFLCTZKcMre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers as tfkl\n","\n","def build_CONV_LSTM_model(input_shape, output_shape):\n","    # Ensure the input time steps are at least as many as the output time steps\n","    assert input_shape[0] >= output_shape[0], \\\n","        \"For this exercise we want input time steps >= output time steps\"\n","\n","    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n","\n","    # 1st LSTM layer\n","    x = tfkl.LSTM(128, return_sequences=True, name='lstm1')(input_layer)\n","    x = tfkl.Dropout(0.3)(x)\n","\n","    # 2nd LSTM layer\n","    x = tfkl.LSTM(128, return_sequences=True, name='lstm2')(x)\n","    x = tfkl.Dropout(0.3)(x)\n","\n","    # 1D Convolution + ReLU\n","    x = tfkl.Conv1D(128, 3, padding='same', name='conv1')(x)\n","    x = tfkl.Activation('relu', name='relu_after_conv1')(x)\n","    x = tfkl.Dropout(0.3)(x)\n","\n","    # 1D Convolution + ReLU\n","    x = tfkl.Conv1D(128, 3, padding='same', name='conv2')(x)\n","    x = tfkl.Activation('relu', name='relu_after_conv2')(x)\n","    x = tfkl.Dropout(0.3)(x)\n","\n","    # Final Convolution => matches desired output's features\n","    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n","\n","    # Crop the time dimension to match output_shape[0]\n","    crop_size = output_layer.shape[1] - output_shape[0]\n","    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n","\n","    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n","    model.compile(\n","        loss=tf.keras.losses.MeanSquaredError(),\n","        optimizer=tf.keras.optimizers.AdamW()\n","    )\n","    return model\n","\n","# Build it with the shapes you have\n","input_shape = (168, 1)  # 168 hours, 1 feature if you're only using NO2\n","output_shape = (24, 1)  # 24 hours forecast, 1 feature\n","\n","model = build_CONV_LSTM_model(input_shape, output_shape)\n","model.summary()\n"],"metadata":{"id":"jo38_eK8cQfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    X_train, y_train,\n","    epochs=10,               # Increase as needed\n","    batch_size=32,           # Adjust to your system's memory\n","    validation_data=(X_val, y_val),\n","    verbose=1\n",")"],"metadata":{"id":"dVYq3xcacSM_"},"execution_count":null,"outputs":[]}]}